# -*- coding: utf-8 -*-
"""stock_price_prediction_version2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXAgAETr9vP7WCFuOkLomRSe3CA7VEfc
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
# Configure data path
DATA_PATH = "/content/drive/My Drive/CS230 Stock Project/data"

"""# **Data Loading**"""

import os
import pandas as pd
import time

# Check if directory exist
# Path = 'D:\XDM Download\Compressed\ReutersNews106521\ReutersNews106521'
path="D:\XDM Download\Compressed\bloombergnews"
print(os.path.isdir(path))

# Helper function traverse directory contains news 
# Returns a interator 
def scanRec(baseDir):
    for entry in os.scandir(baseDir):
        if entry.is_file():
            yield entry.path
        elif entry.is_dir():
            yield from scanRec(entry.path)


# Store all file path in a list
files_path = []
for file_path in scanRec(path):
    files_path.append(file_path)


newsDF = pd.DataFrame(columns = ['title','time','content'])
# File operation
for file_path in files_path:
    # Tempt to read files line by line
    try:
        with open(file_path) as f:
            lines = [line.rstrip() for line in f]
        # Get the title of news
        newsTitle = lines[0][3:] 
        # Format time of the news
        dateTime = lines[2][3:]
        newsTime = time.strptime(dateTime[:-4], '%a %b %d, %Y %I:%M%p')
        # Dump the rest of the news in contents
        newsContent = lines[3:]
        # Store everything in a news dataframe
        newsDF = newsDF.append({'title' : newsTitle, 'time' : newsTime, 'content' : newsContent}, ignore_index=True)
    except Exception:
        print(file_path)

"""# **Prepare Data Labels**"""

# Load stock prices
SP500 = pd.read_csv(DATA_PATH + '/SP500.csv')
SP500.head

# Mark price increase as +1 and price decrease as 0
SP500_daily_return = SP500[['GSPC.Adjusted']].pct_change()
SP500_movement_direction = (SP500_daily_return > 0) * 1
#SP500_movement_direction[SP500_movement_direction == 0] = 0
# Add time information
SP500_movement_direction['time'] = SP500['Unnamed: 0']

SP500_movement_direction

# Store SP500_movement_direction
SP500_movement_direction.to_pickle(DATA_PATH + "/SP500_movement_direction.pkl")

"""# **Open Information Extraction**"""

# Import libraries
import pandas as pd
import pickle
import numpy as np
import matplotlib.pyplot as plt
import pandas_datareader as web
import time
import string

# Load bloomberg dataset
bloomberg = pd.read_pickle(DATA_PATH + "/bloombergTitleTime.pkl")
# Load reuters dataset
reuters = pd.read_pickle(DATA_PATH + "/ReutersNewsTitleTimeOnly.pkl")

# Prepare Open IE
import stanfordnlp
nlp = stanfordnlp.Pipeline()

from openie import StanfordOpenIE

event_tuples = []

# Process titles from Reuters
with StanfordOpenIE() as client:
    for index, row in reuters.iterrows():
        text = row['title']
        #print(text)
        for triple in client.annotate(text):
            triple['time'] = row['time']
            event_tuples.append(triple)
            #print(triple)
        #print("=======================")

# Process titles from Bloomberg
with StanfordOpenIE() as client:
    for index, row in bloomberg.iterrows():
        text = row['title']
        #print(text)
        for triple in client.annotate(text):
            triple['time'] = row['time']
            event_tuples.append(triple)

# Save event tuples
event_tuples.to_pickle(DATA_PATH + "/event-tuples.pkl")

"""# **Word2Vec Embeddings**"""

import numpy as np
import time
#import word2vec
import pickle
import pandas as pd
import gensim
import seaborn as sns
import matplotlib.pyplot as plt

import gensim.downloader as api
info = api.info()
model = api.load("word2vec-google-news-300")

"""# **BERT Embeddings**"""

from bert_serving.client import BertClient
import pandas as pd
import numpy as np

bc = BertClient()
newsTitleDF = pd.read_pickle("/home/ethan/ethanFileServer/google drive/CS230 Stock Project/data/bloombergTitleTime.pkl")
headline_encoding = bc.encode(headlines)
np.save("BERT_heading.npy", headline_encoding, allow_pickle=True)

"""# **Smooth Inverse Frequency (SIF) Embeddings**"""

# Convert data to lists of strings
bloomberg_list = bloomberg.title.values.tolist()
reuters_list = reuters.title.values.tolist()

print(len(bloomberg_list))
print(len(reuters_list))

# Concatenate time stamps
time = bloomberg.time.append(reuters.time, ignore_index= True)

pip install -U fse

from fse import SplitIndexedList
sentence = SplitIndexedList(bloomberg_list, reuters_list)

# Import FAST_VERSION to ensure compilation of cython routins work correctly
from fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH
print(MAX_WORDS_IN_BATCH)
print(FAST_VERSION)

# Import SIF and a pre-trained word embedding model
from fse.models import SIF
import gensim.downloader as api
model = api.load("word2vec-google-news-300")

# Create SIF model
sif = SIF(model)

# Train SIF model on data
sif.train(sentence)

sif_embedding = list()

for i in range(len(time)):
  sif_embedding.append(sif.sv.get_vector(i))

sif_embedding = pd.Series(sif_embedding)

sif_result = {"SIF_embedding": sif_embedding, "Time": time}
sif_result = pd.DataFrame(sif_result)

# Store result of SIF embeddings
sif_result.to_pickle(DATA_PATH + "/sif.pkl")

"""# **Universal Sentence Encoder Embeddings**"""

from absl import logging

import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

module_url = "https://tfhub.dev/google/universal-sentence-encoder/4" #@param ["https://tfhub.dev/google/universal-sentence-encoder/4", "https://tfhub.dev/google/universal-sentence-encoder-large/5"]
model = hub.load(module_url)
print ("module %s loaded" % module_url)
def embed(input):
  return model(input)

logging.set_verbosity(tf.logging.ERROR)

with tf.Session() as session:
    session.run([tf.global_variables_initializer(), tf.tables_initializer()])
    bloomberg_embeddings = session.run(embed(bloomberg_list))
    reuters_embeddings = session.run(embed(reuters_list))

use_embeddings = np.vstack((bloomberg_embeddings, reuters_embeddings))

use_list = []
for i in range(use_embeddings.shape[0]):
  use_list.append(use_embeddings[i])

use_list_series = pd.Series(use_list)
use_result = {"USE_embedding": use_list_series, "Time": time}
use_result = pd.DataFrame(use_result)

use_result

# Store result of USE embeddings
use_result.to_pickle(DATA_PATH + "/use.pkl")

"""# **Visualization of Word2Vec Embedding**"""

df = pd.read_pickle(DATA_PATH + "/wordEmbedingVecDF.pkl")

embeddingDF = df

df

ieDF = pd.read_pickle(DATA_PATH + "/event-tuples.pkl")

ieDF = pd.DataFrame(ieDF)

ieDF[:10]

#zip word and corresponding embedding together and turn it into DataFrame
subject = ieDF[:1000].subject
subjectEmbedding = df[:1000].subject
subjectZip = zip(subject.to_list(), subjectEmbedding.to_list())

relation = ieDF[:1000].relation
relationEmbedding = df[:1000].relation
relationZip = zip(relation.to_list(), relationEmbedding.to_list())

objectNews = ieDF[:1000].object
objectEmbedding = df[:1000].object
objectZip = zip(objectNews.to_list(), objectEmbedding.to_list())

# Cast to a dict so we can turn it into a DataFrame
subjectDict = dict(subjectZip)
relationDict = dict(relationZip)
objectDict = dict(objectZip)

subjectDF = pd.DataFrame.from_dict(subjectDict, orient='index')
relationDF = pd.DataFrame.from_dict(relationDict, orient='index')
objectDF = pd.DataFrame.from_dict(objectDict, orient='index')

from sklearn.manifold import TSNE

# Initialize t-SNE
tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)

tsne_subject = tsne.fit_transform(subjectDF)
tsne_relation = tsne.fit_transform(relationDF)
tsne_object = tsne.fit_transform(objectDF)

sns.set()
# Initialize figure
fig, ax = plt.subplots(figsize = (11.7, 8.27))
sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)

# Import adjustText, initialize list of texts
from adjustText import adjust_text
texts = []
words_to_plot = list(np.arange(0, 400, 10))

# Append words to list
for word in words_to_plot:
    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))
    
# Plot text using adjust_text (because overlapping text is hard to read)
adjust_text(texts, force_points = 0.4, force_text = 0.4, 
            expand_points = (2,1), expand_text = (1,2),
            arrowprops = dict(arrowstyle = "-", color = 'black', lw = 0.5))

plt.show()

!pip install adjustText

#plot in 2D
# Import adjustText, initialize list of texts
from adjustText import adjust_text

def plot_tsne(tsne_df, df, skip = 10):
  sns.set()
  # Initialize figure
  fig, ax = plt.subplots(figsize = (11.7, 8.27))
  sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)

  texts = []
  words_to_plot = list(np.arange(0, tsne_df.shape[0], skip))
  
  # Append words to list
  for word in words_to_plot:
      texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))
      
  # Plot text using adjust_text (because overlapping text is hard to read)
  adjust_text(texts, force_points = 0.4, force_text = 0.4, 
              expand_points = (2,1), expand_text = (1,2),
              arrowprops = dict(arrowstyle = "-", color = 'black', lw = 0.5))
  
  plt.show()

plot_tsne(tsne_relation, relationDF)

plot_tsne(tsne_object, objectDF, skip=20)

plot_tsne(tsne_subject, subjectDF)

"""# **Visualization of USE Embedding**"""

useDF = pd.read_pickle(DATA_PATH + "/sentence_frame.pkl")

use_embedd = pd.read_pickle(DATA_PATH + "/use_embeddings_result.pkl")

#zip word and corresponding embedding together and turn it into DataFrame
sentence = useDF[:1000].Sentence
use_embeddings = use_embedd[:1000].use_embeddings
use_Zip = zip(sentence.to_list(), use_embeddings.to_list())

# Cast to a dict so we can turn it into a DataFrame
use_Dict = dict(use_Zip)

use_DF = pd.DataFrame.from_dict(use_Dict, orient='index')

from sklearn.manifold import TSNE

# Initialize t-SNE
tsne_use = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)

use_tsne= tsne_use.fit_transform(use_DF)

plot_tsne(use_tsne, use_DF,skip=60)

ieDF = pd.read_pickle(DATA_PATH + "/event-tuples.pkl")
ieDF=pd.DataFrame(ieDF)

use_event_embedd = pd.read_pickle(DATA_PATH + "/use_event_embeddings.pkl")

subject = ieDF[:1000].subject
use_subjectEmbedding = use_event_embedd[:1000].USE_subject_embedding
use_subjectZip = zip(subject.to_list(), use_subjectEmbedding.to_list())

relation = ieDF[:1000].relation
use_relationEmbedding = use_event_embedd[:1000].USE_relation_embedding
use_relationZip = zip(relation.to_list(), use_relationEmbedding.to_list())

objectNews = ieDF[:1000].object
use_objectEmbedding = use_event_embedd[:1000].USE_object_embedding
use_objectZip = zip(objectNews.to_list(), use_objectEmbedding.to_list())

# Cast to a dict so we can turn it into a DataFrame
use_subjectDict = dict(use_subjectZip)
use_relationDict = dict(use_relationZip)
use_objectDict = dict(use_objectZip)

use_subjectDF = pd.DataFrame.from_dict(use_subjectDict, orient='index')
use_relationDF = pd.DataFrame.from_dict(use_relationDict, orient='index')
use_objectDF = pd.DataFrame.from_dict(use_objectDict, orient='index')

from sklearn.manifold import TSNE

# Initialize t-SNE
use_tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)

use_tsne_subject = use_tsne.fit_transform(use_subjectDF)
use_tsne_relation = use_tsne.fit_transform(use_relationDF)
use_tsne_object = use_tsne.fit_transform(use_objectDF)

plot_tsne(use_tsne_subject, use_subjectDF)

plot_tsne(use_tsne_relation, use_relationDF)

plot_tsne(use_tsne_object, use_objectDF,skip=20)

plot_tsne(tsne_subject, subjectDF)

"""# **Averaging Word2Vec word embeddings each day**"""

# Load word2vec results
vec = pd.read_pickle(DATA_PATH + "/wordEmbedingVecDF.pkl")
# Sum values of first three columns for each row
vec['vector'] = vec.object + vec.relation + vec.subject
# Drop extraneous columns
vec = vec.drop(['object', 'relation', 'subject'], axis = 1)

# Store vec object
vec.to_pickle(DATA_PATH + "/vec.pkl")

# Remove intra-day time information
from time import gmtime, strftime
for index, row in vec.iterrows():
  row['time'] = strftime("%Y-%m-%d", row['time'])

# Create a new data frame to store average vector at each time
ave_vec = pd.DataFrame(columns=['ave_vec', 'time'])
grouped = vec.groupby('time')
for name, group in grouped:
  curr_time = name
  all_vec = group['vector']
  all_vec = all_vec.to_numpy()
  curr_ave = np.mean(all_vec)
  ave_vec = ave_vec.append({'time': curr_time, 'ave_vec': curr_ave}, ignore_index=True)

# Store ave_vec object
ave_vec.to_pickle(DATA_PATH + "/ave_vec.pkl")

"""# **Average Bert embeddings each day**"""

vec = pd.read_pickle(DATA_PATH+'YZ_Bert_formattedData.pkl')

ave_vec_bert=[]
for i in range(502):
  ave_vec_bert.append(np.mean(vec['embeddings'][i],axis=0))
label_bert=vec.iloc[:,0].values.tolist()
label_bert=pd.DataFrame(label_bert,columns=['label'])

ave_vec_bert=pd.Series(ave_vec_bert)
ave_vec_bert=pd.DataFrame(ave_vec_bert,columns=['ave_vec'])

time=vec.index.values.tolist()
time=pd.DataFrame(time,columns=['time'])

ave_vec_bert2=pd.concat([ave_vec_bert,label_bert,time],axis=1)

ave_vec_bert2.to_pickle(DATA_PATH+'/ave_vec_bert.pkl')

"""# **Average SIF embeddings each day**"""

import pandas as pd
import pickle
import numpy as np

vec = pd.read_pickle(DATA_PATH + "/sif.pkl")
# Remove intra-day time information
import os
import pandas as pd
import time
from time import gmtime, strftime
for index, row in vec.iterrows():
  row['Time'] = strftime("%Y-%m-%d", row['Time'])
# Create a new data frame to store average vector at each time
ave_vec = pd.DataFrame(columns=['ave_vec', 'time'])
grouped = vec.groupby('Time')

for name, group in grouped:
  curr_time = name
  all_vec = group['SIF_embedding']
  all_vec = all_vec.to_numpy()
  curr_ave = np.mean(all_vec)
  ave_vec = ave_vec.append({'time': curr_time, 'ave_vec': curr_ave}, ignore_index=True)
# Store ave_vec object
ave_vec.to_pickle(DATA_PATH + "/ave_vec_sif.pkl")

"""# **Average USE embeddings each day**"""

vec = pd.read_pickle(DATA_PATH + "/use.pkl")
# Remove intra-day time information
import os
import pandas as pd
import time
from time import gmtime, strftime
for index, row in vec.iterrows():
  row['Time'] = strftime("%Y-%m-%d", row['Time'])
# Create a new data frame to store average vector at each time
ave_vec = pd.DataFrame(columns=['ave_vec', 'time'])
grouped = vec.groupby('Time')

for name, group in grouped:
  curr_time = name
  all_vec = group['USE_embedding']
  all_vec = all_vec.to_numpy()
  curr_ave = np.mean(all_vec)
  ave_vec = ave_vec.append({'time': curr_time, 'ave_vec': curr_ave}, ignore_index=True)
# Store ave_vec object
ave_vec.to_pickle(DATA_PATH + "/ave_vec_use.pkl")
ave_vec

"""# **Remove Weekend and Holiday Data**"""

all_data = pd.read_pickle(DATA_PATH + "/ave_vec_sif.pkl")

all_data.iloc[[0,3]]

SP500_movement_direction = pd.read_pickle(DATA_PATH + "/SP500_movement_direction.pkl")
real_time=set(SP500_movement_direction['time'])
del_list=[]
for i in range(len(all_data['time'])):
  if not all_data['time'][i] in real_time:
    del_list.append(i)

data=all_data.drop(del_list)
data=data.reset_index(drop=True)

data

"""# Logistic Regression"""

import sklearn
from sklearn import metrics 
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, cross_val_score

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold # import KFold
from sklearn import datasets

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_fscore_support

X = list(data['ave_vec'])
y = list(SP500_movement_direction['GSPC.Adjusted'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LogisticRegression(solver="lbfgs")
model.fit(X_train, y_train)

#model evaluation
train_score = model.score(X_train, y_train)
y_pred_train = model.predict(X_train)
train_precision, train_recall, train_fscore, train_support = precision_recall_fscore_support(y_train, y_pred_train, average="binary")
test_score = model.score(X_test, y_test)
y_pred_test = model.predict(X_test)
test_precision, test_recall, test_fscore, test_support = precision_recall_fscore_support(y_test, y_pred_test, average="binary")

result = {
  'train_accuracy': train_score, 
  'train_precision': train_precision, 
  'train_recall': train_recall, 
  'train_fscore': train_fscore, 
  'test_accuracy': test_score, 
  'test_precision': test_precision, 
  'test_recall': test_recall, 
  'test_fscore': test_fscore,
}

print(result)

training_dataset={} #a dictionary, key: date, value: training value
val_dataset={}
lt_days=30
mt_days=7
for i in range(lt_days,1407):
  lt=np.array(data['ave_vec'][i-lt_days:i])
  mt=np.array(data['ave_vec'][i-mt_days:i])
  st=np.array(data['ave_vec'][i-1])
  label=SP500_movement_direction['GSPC.Adjusted	'][i]
  training_dataset[data['time'][i]]=[lt,mt,st,label]
for i in range(1407,len(data['time'])):
  lt=np.array(data['ave_vec'][i-lt_days:i])
  mt=np.array(data['ave_vec'][i-mt_days:i])
  st=np.array(data['ave_vec'][i-1])
  label=SP500_movement_direction['GSPC.Adjusted	'][i]
  val_dataset[data['time'][i]]=[lt,mt,st,label]

"""# **CNN1**"""

# Commented out IPython magic to ensure Python compatibility.
import json
import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
#import cPickle as pickle
import pickle
import os
from datetime import datetime, timedelta
import pandas as pd

xrange=range
DATA_PATH_PIC = "/content/drive/My Drive/CS230 Stock Project/data/PIC/batch_size1_version1/" #store checkpoints

'''
  params:
        lt_days: meaning: number of long term days
                 contained function: _init_ (with a diff name: self.max_long_term),main
        mt_days: meaning: number of mid term days
                 contained function: _init_ (with a diff name: self.max_mid_term),main
        self.d: meaning: size of word embedding vector
                contained function: _init_
        word embeddings: 
                                    contained function: main; 
                                    change method: load data from different pkl files
        
        

'''



class CNN():

    def __init__(self, sess):
        
        self.sess = sess
        self.epochs = 100
        self.batch_size =10

        self.l2_lambda = 1e-4
        self.lr = 1e-4

        self.d = 300
        self.l = 3

        self.max_mid_term = 7
        self.max_long_term = 30

        self.checkpoint_dir = '/content/drive/My Drive/CS230 Stock Project/checkpoints/'
        self.build_model()

    def build_model(self):
        #define input layers
        self.long_term_events = tf.placeholder(tf.float32, [self.batch_size, self.max_long_term, self.d], name='long_term_events')
        self.mid_term_events = tf.placeholder(tf.float32, [self.batch_size, self.max_mid_term, self.d], name='mid_term_events')
        self.short_term_events = tf.placeholder(tf.float32, [self.batch_size, None, self.d], name='short_term_events')
        
        #define output layers
        self.labels = tf.placeholder(tf.float32, [self.batch_size], name='labels')

        #define hidden layers/convolution layers
        self.long_term_conv = self.conv1d(self.long_term_events, output_dim=self.d, name='long_term_conv')
        self.mid_term_conv = self.conv1d(self.mid_term_events, output_dim=self.d, name='mid_term_conv')

        #define pooling layers
        self.long_term_pool = tf.reduce_max(self.long_term_conv, axis=2)
        
      
        self.mid_term_pool = tf.reduce_max(self.mid_term_conv, axis=2)
        self.short_term_pool = tf.reduce_mean(self.short_term_events, axis=1)

        #concatenate long_term, mid_term, and short_term pool into one tensor along the num_event axis 1
        self.v = tf.concat([self.long_term_pool, self.mid_term_pool, self.short_term_pool], axis=1)
 
        #fully connected hidden layers with 100 variables
        self.y = tf.layers.dense(self.v, 100, activation='relu')
        # print self.y.shape

        #output layer
        self.logits = tf.reshape(tf.layers.dense(self.y, 1, activation='sigmoid'), [self.batch_size], name='preds')
        # print self.logits.shape

        # print tf.trainable_variables()
        
        # ce_loss: regular old cross entropy loss; l2_loss: minimize square errors
        self.ce_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.logits))
        self.l2_loss = sum([tf.reduce_sum(tf.square(var)) for var in tf.trainable_variables()])

        self.loss = self.ce_loss + (self.l2_lambda * self.l2_loss)

        self.saver = tf.train.Saver()

    # Yitian: Sanity check of params
    # param input: the input layer
    # param output_dim: the number of filters to use
    # param k_w: kernel width
    # param d_w: stride 
    def conv1d(self, input, output_dim=1, k_w=3, d_w=1, stddev=0.02, name='conv1d'):
        
        
        with tf.variable_scope(name):

            w = tf.get_variable('w', [k_w, input.get_shape()[-1], output_dim],
                                initializer=tf.truncated_normal_initializer(stddev=stddev))
            conv = tf.nn.conv1d(input, w, stride=d_w, padding='SAME')

            biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))
            conv = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))
            #what does tf.reshape here do? Seems redundant
            return conv


    def train(self, data, val_data):
        cnn_optim = tf.train.AdamOptimizer(self.lr).minimize(self.loss, var_list=tf.trainable_variables())

        init_op = tf.global_variables_initializer()
        self.sess.run(init_op)

        counter = 0
        start_time = time.time()

        losses = []
        ce_losses = []
        l2_losses = []
        tps = []
        fps = []
        tns = []
        fns = []

        for epoch in xrange(self.epochs):
            length = len(data)
            batch_idxs = length // self.batch_size

            batch_loss = 0
            batch_ce_loss = 0
            batch_l2_loss = 0
            batch_per_error = 0
            batch_tp = 0
            batch_fp = 0
            batch_tn = 0
            batch_fn = 0

            for idx in xrange(0, batch_idxs):
                batch_data = data[idx*self.batch_size:(idx+1)*self.batch_size]
                feed_dict = {   
                                self.short_term_events: np.array([batch_data[i][0] for i in range(len(batch_data))]).reshape([self.batch_size,1, self.d]),
                                self.mid_term_events: np.array([batch_data[i][1] for i in range(len(batch_data))]),
                                self.long_term_events: np.array([batch_data[i][2] for i in range(len(batch_data))]),
                                self.labels: np.array([batch_data[i][3] for i in range(len(batch_data))])
                            }
                vlist = tf.trainable_variables()
                vnames = [v.name for v in vlist]
                
                # print vnames
                __ = self.sess.run([cnn_optim, self.logits] + vlist, feed_dict=feed_dict)
                
                _ = __[0]
                logits = __[1]
                
                label = [batch_data[i][3] for i in range(len(batch_data))]
                ce_loss = self.ce_loss.eval(feed_dict)
                l2_loss = self.l2_loss.eval(feed_dict)
                loss = self.loss.eval(feed_dict)

                # print loss, u_loss, l2_loss
                batch_loss += loss
                batch_ce_loss += ce_loss
                batch_l2_loss += l2_loss
          
                for z in range(len(label)):

                  if label[z] == 1 and logits[z] > .5:
                      batch_tp += 1.0
                  if label[z] == 0 and logits[z] > .5:
                      batch_fp += 1.0
                  if label[z] == 0 and logits[z] <= .5:
                      batch_tn += 1.0
                  if label[z] == 1 and logits[z] <= .5:
                      batch_fn += 1.0
                
                
                counter += 1
                print('Train Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f, ce_loss: %.8f, l2_loss: %.8f' \
#                     % (epoch, idx, batch_idxs,
                        time.time() - start_time, loss, ce_loss, l2_loss))           
              
                print (logits, feed_dict[self.labels])

            batch_loss /= batch_idxs
            batch_ce_loss /= batch_idxs
            batch_l2_loss /= batch_idxs
            losses += [[batch_loss]]
            ce_losses += [[batch_ce_loss]]
            l2_losses += [[self.l2_lambda * batch_l2_loss]]
            tps += [[batch_tp]]
            fps += [[batch_fp]]
            tns += [[batch_tn]]
            fns += [[batch_fn]]
            # print losses

# -------------------VALIDATION----------------------

            # val_length = min(len(val_data), self.val_size)
            val_length = len(val_data)
            val_batch_idxs = val_length // self.batch_size

            batch_loss = 0
            batch_ce_loss = 0
            batch_l2_loss = 0
            batch_per_error = 0
            batch_tp = 0
            batch_fp = 0
            batch_tn = 0
            batch_fn = 0

            for idx in xrange(0, val_batch_idxs):
                batch_data = val_data[idx*self.batch_size:(idx+1)*self.batch_size]
                feed_dict = {   
                                self.short_term_events: np.array([batch_data[i][0] for i in range(len(batch_data))]).reshape([self.batch_size,1, self.d]),
                                self.mid_term_events: np.array([batch_data[i][1] for i in range(len(batch_data))]),
                                self.long_term_events: np.array([batch_data[i][2] for i in range(len(batch_data))]),
                                self.labels: np.array([batch_data[i][3] for i in range(len(batch_data))])
                            }
                
                
                ce_loss = self.ce_loss.eval(feed_dict)
                
                l2_loss = self.l2_loss.eval(feed_dict)
                loss = self.loss.eval(feed_dict)
                logits = self.logits.eval(feed_dict)

                label = batch_data[0][3].tolist()
                
                # print loss, u_loss, l2_loss
                batch_loss += loss
                batch_ce_loss += ce_loss
                batch_l2_loss += l2_loss
               
                if np.isscalar(label):
                   label=[label]
                for z in range(len(label)):
                      if label[z] == 1 and logits[z] > .5:
                          batch_tp += 1.0
                      if label[z] == 0 and logits[z] > .5:
                          batch_fp += 1.0
                      if label[z] == 0 and logits[z] <= .5:
                          batch_tn += 1.0
                      if label[z] == 1 and logits[z] <= .5:
                          batch_fn += 1.0
                  
                

                print('Val Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f, ce_loss: %.8f, l2_loss: %.8f' \
#                     % (epoch, idx, val_batch_idxs,
                        time.time() - start_time, loss, ce_loss, l2_loss))

            batch_loss /= val_batch_idxs
            batch_ce_loss /= val_batch_idxs
            batch_l2_loss /= val_batch_idxs
            losses[-1] += [batch_loss]
            ce_losses[-1] += [batch_ce_loss]
            l2_losses[-1] += [self.l2_lambda * batch_l2_loss]
            tps[-1] += [batch_tp]
            fps[-1] += [batch_fp]
            tns[-1] += [batch_tn]
            fns[-1] += [batch_fn]

            losses_arr = np.array(losses)
            ce_losses_arr = np.array(ce_losses)
            l2_losses_arr = np.array(l2_losses)

            tps_arr = np.array(tps)
            fps_arr = np.array(fps)
            tns_arr = np.array(tns)
            fns_arr = np.array(fns)


            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(losses_arr))], losses_arr[:,0], label='total loss', color='b')
            plt.plot([i+1 for i in range(len(ce_losses_arr))], ce_losses_arr[:,0], label='ce loss', color='g')
            plt.plot([i+1 for i in range(len(l2_losses_arr))], l2_losses_arr[:,0], label='l2 loss', color='r')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'trn_losses.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(tps_arr))], tps_arr[:,0], label='tp', color='b')
            plt.plot([i+1 for i in range(len(fps_arr))], fps_arr[:,0], label='fp', color='g')
            plt.plot([i+1 for i in range(len(tns_arr))], tns_arr[:,0], label='tn', color='r')
            plt.plot([i+1 for i in range(len(fns_arr))], fns_arr[:,0], label='fn', color='c')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'trn_pns.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide((tps_arr+tns_arr)[:,0], (tps_arr+tns_arr+fps_arr+fns_arr)[:,0]), label='acc', color='b')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,0], (tps_arr+fps_arr)[:,0]), label='precision', color='g')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,0], (tps_arr+fns_arr)[:,0]), label='recall', color='r')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'trn_acc.png')


            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(losses_arr))], losses_arr[:,1], label='total loss', color='b')
            plt.plot([i+1 for i in range(len(ce_losses_arr))], ce_losses_arr[:,1], label='ce loss', color='g')
            plt.plot([i+1 for i in range(len(l2_losses_arr))], l2_losses_arr[:,1], label='l2 loss', color='r')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'val_losses.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(tps_arr))], tps_arr[:,1], label='tp', color='b')
            plt.plot([i+1 for i in range(len(fps_arr))], fps_arr[:,1], label='fp', color='g')
            plt.plot([i+1 for i in range(len(tns_arr))], tns_arr[:,1], label='tn', color='r')
            plt.plot([i+1 for i in range(len(fns_arr))], fns_arr[:,1], label='fn', color='c')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'val_pns.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide((tps_arr+tns_arr)[:,1], (tps_arr+tns_arr+fps_arr+fns_arr)[:,1]), label='acc', color='b')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,1], (tps_arr+fps_arr)[:,1]), label='precision', color='g')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,1], (tps_arr+fns_arr)[:,1]), label='recall', color='r')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'val_acc.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(losses_arr))], losses_arr[:,0], label='train total loss', color='b')
            plt.plot([i+1 for i in range(len(ce_losses_arr))], ce_losses_arr[:,0], label='train ce loss', color='g')
            plt.plot([i+1 for i in range(len(l2_losses_arr))], l2_losses_arr[:,0], label='train l2 loss', color='r')
            plt.plot([i+1 for i in range(len(losses_arr))], losses_arr[:,1], label='val total loss', linestyle='--', color='b')
            plt.plot([i+1 for i in range(len(ce_losses_arr))], ce_losses_arr[:,1], label='val ce loss', linestyle='--', color='g')
            plt.plot([i+1 for i in range(len(l2_losses_arr))], l2_losses_arr[:,1], label='val l2 loss', linestyle='--', color='r')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'losses.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(tps_arr))], tps_arr[:,0], label='train tp', color='b')
            plt.plot([i+1 for i in range(len(fps_arr))], fps_arr[:,0], label='train fp', color='g')
            plt.plot([i+1 for i in range(len(tns_arr))], tns_arr[:,0], label='train tn', color='r')
            plt.plot([i+1 for i in range(len(fns_arr))], fns_arr[:,0], label='train fn', color='c')
            plt.plot([i+1 for i in range(len(tps_arr))], tps_arr[:,1], label='val tp', linestyle='--', color='b')
            plt.plot([i+1 for i in range(len(fps_arr))], fps_arr[:,1], label='val fp', linestyle='--', color='g')
            plt.plot([i+1 for i in range(len(tns_arr))], tns_arr[:,1], label='val tn', linestyle='--', color='r')
            plt.plot([i+1 for i in range(len(fns_arr))], fns_arr[:,1], label='val fn', linestyle='--', color='c')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'pns.png')

            plt.figure(figsize=(16,10))
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide((tps_arr+tns_arr)[:,0], (tps_arr+tns_arr+fps_arr+fns_arr)[:,0]), label='train acc', color='b')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,0], (tps_arr+fps_arr)[:,0]), label='train precision', color='g')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,0], (tps_arr+fns_arr)[:,0]), label='train recall', color='r')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide((tps_arr+tns_arr)[:,1], (tps_arr+tns_arr+fps_arr+fns_arr)[:,1]), label='val acc', linestyle='--', color='b')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,1], (tps_arr+fps_arr)[:,1]), label='val precision', linestyle='--', color='g')
            plt.plot([i+1 for i in range(len(tps_arr))], np.divide(tps_arr[:,1], (tps_arr+fns_arr)[:,1]), label='val recall', linestyle='--', color='r')
            plt.legend()
            plt.savefig(DATA_PATH_PIC+'acc.png')



def main(_):
    SP500_movement_direction = pd.read_pickle(DATA_PATH + "/SP500_movement_direction.pkl")
    all_data = pd.read_pickle(DATA_PATH + "/ave_vec_sif.pkl") #place to change word embedding method
    real_time=set(SP500_movement_direction['time'])
    del_list=[]
    for i in range(len(all_data['time'])):
      if not all_data['time'][i] in real_time:
        del_list.append(i)

    data=all_data.drop(del_list)
    data=data.reset_index(drop=True)
    print(len(data['ave_vec'][0]))

    training_dataset=[]
    val_dataset=[]
    lt_days=30 #long term days number
    mt_days=7 #mid term days number
    for i in range(lt_days,int(len(data['time'])*0.8)):
      lt=data['ave_vec'][i-lt_days:i]
      mt=data['ave_vec'][i-mt_days:i]
      st=data['ave_vec'][i-1]
      label=SP500_movement_direction['GSPC.Adjusted'][i]
      #label=data['label'][i]
      training_dataset.append([st,mt,lt,label])
    for i in range(int(len(data['time'])*0.8),len(data['time'])):
      lt=data['ave_vec'][i-lt_days:i]
      mt=data['ave_vec'][i-mt_days:i]
      st=data['ave_vec'][i-1]
      label=SP500_movement_direction['GSPC.Adjusted'][i]
      #label=data['label'][i]
      val_dataset.append([st,mt,lt,label])
    tf.reset_default_graph()
    with tf.Session() as sess:
          model = CNN(sess)
          model.train(training_dataset, val_dataset)

if __name__ == '__main__':
    tf.app.run()

"""# **CNN2**"""

import random
import numpy as np
import operator
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D,AveragePooling2D
from keras.layers.convolutional import Convolution1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.utils.np_utils import to_categorical
from sklearn.metrics import confusion_matrix
'''
  params to adjust: word embeddings: 
                                    contained function: get_Feature_Label; 
                                    change method: load data from different pkl files
                    lt_days:
                            meaning: the number of past days we are using
                            contained function: CNN,get_Feature_Label
                            
                    wb_size:
                            meaning: the size of one word embedding vector
                            contained function:CNN,get_Feature_Label
                            
                    thres: 
                          meaning: the bar of outputting prediction as 0 or 1 (an important parameter)
                          contained function: evaluate
                          
  '''

def value2int(y, clusters=2):
  '''not used'''
    label = np.copy(y)
    label[y < np.percentile(y, 100 / clusters)] = 0
    for i in range(1, clusters):
        label[y > np.percentile(y, 100 * i / clusters)] = i
    return label

def value2int_simple(y):
  '''not used'''
    label = np.copy(y)
    label[y <=0] = 0
    label[y > 0] = 1
    return label

def get_Feature_Label(clusters=2, hasJunk=True):
    SP500_movement_direction = pd.read_pickle(DATA_PATH + "/SP500_movement_direction.pkl")
    # all_data = pd.read_pickle(DATA_PATH + "/ave_vec.pkl")   
    all_data = pd.read_pickle(DATA_PATH + "/ave_vec_sif.pkl")    #  place to change word embeddings "ave_vec_sif is the word embedding gotten from sif"
    real_time=set(SP500_movement_direction['time'])
    del_list=[]
    for i in range(len(all_data['time'])):
      #if (not all_data['time'][i] in real_time) or not (all_data['time'][i].startswith("2012") or all_data['time'][i].startswith("2013")): #using all data or data from 2012 to 2013
      if (not all_data['time'][i] in real_time) :  #remove holidays and weekends
        del_list.append(i)
    data=all_data.drop(del_list)
    data=data.reset_index(drop=True)

    x_train=[]
    y_train=[]
    x_val=[]
    y_val=[]
    val_dataset=[]
    lt_days=20  #the number of past days we are using
    wb_size=300  #size of one word embedding vector
    #getting training data 
    for i in range(lt_days,int(len(data['time'])*0.8)):
      lt=data['ave_vec'][i-lt_days:i]
      label=SP500_movement_direction['GSPC.Adjusted'][i]
      #label=data['label'][i]
      x_train.append(lt)
      y_train.append(label)
    x_train=np.array(x_train)
    y_train2=np.array(y_train)
    y_train= to_categorical(value2int_simple(y_train2)).astype("int")
    #getting validation data
    for i in range(int(len(data['time'])*0.8),len(data['time'])):
      lt=data['ave_vec'][i-lt_days:i]
      label=SP500_movement_direction['GSPC.Adjusted'][i]
      #label=data['label'][i]
      x_val.append(lt)
      y_val.append(label)
    x_val=np.array(x_val)
    y_val2=np.array(y_val)
    y_val= to_categorical(value2int_simple(y_val2)).astype("int") 
    x_train = x_train.reshape(x_train.shape[0], lt_days, wb_size, 1).astype('float32')
    x_val = x_val.reshape(x_val.shape[0], lt_days, wb_size, 1).astype('float32')


    return x_train, y_train, x_val, y_val


def CNN(clusters):
    '''
    paper: 61.73%
    data: all from 2006-2013 (data makes a hugh diff)
    all predict to increase: accuracy 56.7%
    sif: 0.58 accuracy 59%
    word2Vvec: 0.50(no differ from 0.3-0.52) 56.7%
    '''
    lt_days=20  #the number of past days we are using
    wb_size=300  #size of one word embedding vector
    model = Sequential()
    model.add(Convolution2D(64, 3, wb_size, border_mode='valid', input_shape=(lt_days, wb_size, 1), activation='relu')) 
    
    model.add(AveragePooling2D(pool_size=(3, 1)))
    
    model.add(Dropout(0.4))
    model.add(Flatten())
    
    model.add(Dense(100, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(clusters, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def evaluate(model, clusters, x_train, y_train, x_val, y_val):
    model.fit(x_train, y_train, nb_epoch=20, batch_size=110, verbose=2)
    
    # Final evaluation of the model
    score = model.evaluate(x_val, y_val, verbose=0)
    

    predictions = model.predict(x_val)
    ##thres is the bar of outputting prediction as 0 or 1 (an important parameter)
    thres = 0.58; y_cut = (predictions[:,0] > thres) | (predictions[:,1] > thres) # cut y value and leave the better result
    predictions = np.argmax(predictions[y_cut], axis=-1)
    conf = confusion_matrix(np.argmax(y_val[y_cut], axis=-1), predictions) #confusion matrix
    print("Test on %d samples" % (len(y_val[y_cut])))
    print(conf)
    diag=0
    for i in range(clusters):
        print("Test Label %d Precision, %.2f%%" % (i, conf[i,i] * 100.0 / sum(conf[:,i])))
        diag+=conf[i,i]
    print("Test Accuracy, %.2f%%" % (diag * 100.0 / sum(sum(conf))))


def model_selection(clusters): 
  '''
  clusters: output size of CNN
  '''
    #get train and test/validation data (we are not using validation)
    X_train, y_train, X_test, y_test = get_Feature_Label(clusters=clusters)

    for i in range(1):
        print("Trial:", i)
        model = CNN(clusters) 
        evaluate(model, clusters, X_train, y_train, X_test, y_test)
        

def main():
    clusters = 2
    model_selection(clusters)
    


if __name__ == "__main__":
    main()

"""# **Multilayer Perceptron**"""

from sklearn.neural_network import MLPRegressor

"""## **RNN**"""

import pandas as pd
import numpy as np
import datetime
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Bidirectional
from keras.layers import concatenate
from keras.layers import TimeDistributed
from keras.layers import Lambda
from keras.layers import Flatten
from keras.layers import GRU
from keras.layers import Conv1D
from keras.layers import MaxPooling1D
from keras.models import Sequential
from keras.models import Model
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.backend import stack
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

"""BERT"""

#BERT data preprocessing
formatted_training = pd.read_pickle(DATA_PATH + "YZ_Bert_formattedDataPaded.pkl")
X_train = np.array(list(formatted_training['pad_embeddings']))
y_train = np.array(list(formatted_training[0]))

formatted_training

#3 day memory for LSTM
#day 1
X_train_1 = X_train

#day 2
X_train_2 = X_train[1:]
X_train_2 = np.append(X_train_2, [X_train[0]], axis=0)

#day 3
X_train_3 = X_train[2:]
X_train_3 = np.append(X_train_3, X_train[:2], axis=0)

y_train_3 = y_train[2:]
y_train_3 = np.append(y_train_3, y_train[:2])

"""SIF"""

#load preprocessed data
X_train = np.load(DATA_PATH + "/SIF_formatted_for_sequence_model_all_time.npy", allow_pickle=True)
y_train = np.load(DATA_PATH + "/sequence_model_y_formatted_all_time.npy", allow_pickle=True)

sif = pd.read_pickle(DATA_PATH + "/sif.pkl")

sif

frequency = pd.read_csv(DATA_PATH + "/time_frequency.csv")
high_frequency = frequency[frequency['Index'] > 10]

#convert Time field into date field in SIF and append to the dataframe
date = [datetime.date(test_time.tm_year, test_time.tm_mon, test_time.tm_mday) for test_time in sif['Time']]
sif['date'] = date

#formatting y value
sp500 = pd.read_pickle(DATA_PATH + "/SP500_movement_direction.pkl")

training_y = {}
for index, row in sp500.iterrows():
  if high_frequency['Date'].str.contains(row['time']).any():
      training_y[row['time']] = row['GSPC.Adjusted']

del training_y['2013-11-26']
y_train = list(training_y.values())

#select continuous data for sequence model
training_X = {}
start_date = datetime.date(2010, 4, 26)
end_date = datetime.date(2013, 11, 26)
day_delta = datetime.timedelta(days=1)

for i in range((end_date - start_date).days):
  curr = start_date + i*day_delta
  training_X[curr.strftime('%Y-%m-%d')]= sif[sif['date'] == curr]

#reshape the data into 3D array; and also padding the data to have same length input
X_train = []
for date in training_y.keys():
  embed = np.array(list(training_X[date]['SIF_embedding']))
  if embed.shape[0] < 500:
    #padding with extra rows of zeros
    embed = np.append(embed, np.zeros((500 - embed.shape[0], 300)), axis=0)
  else:
    embed = embed[:500]
  X_train.append(embed)

X_train = np.array(X_train)

print(len(y_train))
print(X_train.shape)

np.save(DATA_PATH + "/SIF_formatted_for_sequence_model_all_time", X_train, allow_pickle=True)
np.save(DATA_PATH + "/sequence_model_y_formatted_all_time", y_train, allow_pickle=True)

X_test.shape

"""Universal Sentence Embedding"""

X_train = np.load(DATA_PATH + "/USE_formatted_for_sequence_model_all_time.npy", allow_pickle=True)
y_train = np.load(DATA_PATH + "/USE_sequence_model_y_formatted_all_time.npy", allow_pickle=True)

use = pd.read_pickle(DATA_PATH + "/use.pkl")

use

date = [datetime.date(test_time.tm_year, test_time.tm_mon, test_time.tm_mday) for test_time in use['Time']]
use['date'] = date

frequency = pd.read_csv(DATA_PATH + "/time_frequency.csv")
high_frequency = frequency[frequency['Index'] > 10]

#formatting y value
sp500 = pd.read_pickle(DATA_PATH + "/SP500_movement_direction.pkl")

training_y = {}
for index, row in sp500.iterrows():
  if high_frequency['Date'].str.contains(row['time']).any():
      training_y[row['time']] = row['GSPC.Adjusted']

del training_y['2013-11-26']
y_train = list(training_y.values())

#select continuous data for sequence model
training_X = {}
start_date = datetime.date(2010, 4, 26)
end_date = datetime.date(2013, 11, 26)
day_delta = datetime.timedelta(days=1)

for i in range((end_date - start_date).days):
  curr = start_date + i*day_delta
  training_X[curr.strftime('%Y-%m-%d')]= use[use['date'] == curr]

#reshape the data into 3D array; and also padding the data to have same length input
X_train = []
for date in training_y.keys():
  embed = np.array(list(training_X[date]['USE_embedding']))
  if embed.shape[0] < 500:
    #padding with extra rows of zeros
    embed = np.append(embed, np.zeros((500 - embed.shape[0], 512)), axis=0)
  else:
    embed = embed[:500]
  X_train.append(embed)

X_train = np.array(X_train)

print(len(y_train))
print(X_train.shape)

np.save(DATA_PATH + "/USE_formatted_for_sequence_model_all_time", X_train, allow_pickle=True)
np.save(DATA_PATH + "/USE_sequence_model_y_formatted_all_time", y_train, allow_pickle=True)

y_train = np.array(y_train)
y_train.shape

"""## Models"""

def create_lstm(shape=(500, 768), output=32, regress=False):
  model = Sequential()
  model.add(Bidirectional(LSTM(128, return_sequences=True, dropout= 0.2), input_shape=shape))
  model.add(Bidirectional(LSTM(64, dropout=0.2)))
  model.add(Dense(output, activation='sigmoid'))
  if regress:
    model.add(Dense(1, activation='sigmoid'))
  
  return model

#previous model is overfitting too much, simplify hidden layer
def create_lstm(shape=(500, 768), output=32, hidden_unit=64, regress=False):
  model = Sequential()
  model.add(Bidirectional(LSTM(hidden_unit, dropout= 0.2), input_shape=shape))
  model.add(Dense(output, activation='sigmoid'))
  if regress:
    model.add(Dense(1, activation='sigmoid'))
  
  return model

def create_gru(shape=(500, 768), output=32, hidden_unit=64, regress=False):
  model = Sequential()
  model.add(Bidirectional(GRU(hidden_unit, return_sequences=True, dropout= 0.2, kernel_regularizer=l2(0.01)), input_shape=shape))
  model.add(Flatten())
  model.add(Dense(output, activation='sigmoid', kernel_regularizer=l2(0.01)))
  if regress:
    model.add(Dense(1, activation='sigmoid'))
  
  return model

def create_cnnLSTM(shape=(500, 768), output=32, filters=64, kernel_size=5, pool_size=4, hidden_unit=70, regress=False):
  model = Sequential()
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=shape, kernel_regularizer=l2(0.01)))
  model.add(MaxPooling1D(pool_size=pool_size))
  model.add(LSTM(hidden_unit, kernel_regularizer=l2(0.01)))
  model.add(Dense(output, activation='sigmoid'))
  if regress:
    model.add(Dense(1, activation='sigmoid'))

  return model

#create multiday input continuous dataset
def create_multiday(X_train, y_train, num_days = 3):
  Dict = {}
  num_train = X_train.shape[0]
  for i in range(num_days):
    Dict["X_train_" + str(i)] = X_train[i:i-num_days]
  Dict["y_train"] = y_train[num_days-1:-1]
  return Dict

#split test train set
def train_test_split_sqs(data, ratio=0.2):
  train = {}
  test = {}
  num_data = data['y_train'].shape[0]
  num_training = int(num_data*(1-ratio))
  for key in data.keys():
    train[key] = data[key][:num_training]
    test[key] = data[key][num_training:]
  return train, test

# Visualize training history
import matplotlib.pyplot as plt
def plot_learning_curve(history):
  # list all data in history
  print(history.history.keys())
  # summarize history for accuracy
  plt.plot(history.history['acc'])
  plt.plot(history.history['val_acc'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()
  # summarize history for loss
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

# two layers of LSTM, use single day news title, no memory of previous days whatsoever
model_1 = create_lstm(shape=(500,300), regress=True)
model_1.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_1.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.2)

plot_learning_curve(history)

lstm_1 = create_lstm(shape=(500,300))
lstm_2 = create_lstm(shape=(500,300))
lstm_3 = create_lstm(shape=(500,300))

combinedInput = concatenate([lstm_1.output, lstm_2.output, lstm_3.output])

x = Dense(16, activation='relu')(combinedInput)
x = Dense(1, activation='sigmoid')(x)

model_2 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input], outputs=x)

model_2.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_2.fit([X_train_1, X_train_2, X_train_3], y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.2, shuffle=False)
plot_learning_curve(history)

lstm_1 = create_lstm(shape=(500,300), output=4)
lstm_2 = create_lstm(shape=(500,300), output=4)
lstm_3 = create_lstm(shape=(500,300), output=4)
output_list = [lstm_1.output, lstm_2.output, lstm_3.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(3, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_2 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input], outputs=x)

model_2.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_2.fit([X_train_1, X_train_2, X_train_3], y_train, batch_size=32, epochs=100, verbose=1, validation_split=0.2, shuffle=False)
plot_learning_curve(history)

#single layer LSTM
model_3 = create_lstm(shape=(500,300), output=16, regress=True)
model_3.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

#split test train set
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

#fitting model
history = model_3.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.1)
plot_learning_curve(history)

#test model
y_pred = model_3.predict(X_test, batch_size=32, verbose=1)
print(classification_report(y_test, y_pred, labels=['down', 'up']))

#test model
y_pred = model_3.predict(X_test, batch_size=32, verbose=1)

print(classification_report(y_test, np.rint(y_pred.reshape(y_pred.shape[0]))))

#get three days of data
data = create_multiday(X_train, y_train, num_days=3)

trainDict, testDict = train_test_split_sqs(data)

for key in trainDict.keys():
  print(trainDict[key].shape)

#model 4 different output weights
lstm_1 = create_lstm(shape=(500,300), output=4)
lstm_2 = create_lstm(shape=(500,300), output=8)
lstm_3 = create_lstm(shape=(500,300), output=32)

combinedInput = concatenate([lstm_1.output, lstm_2.output, lstm_3.output])

x = Dense(16, activation='relu')(combinedInput)
x = Dense(1, activation='sigmoid')(x)

model_4 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input], outputs=x)

model_4.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

#fitting the model
#removing y_train from dict
#y_train = trainDict['y_train']
#del trainDict['y_train']
history = model_4.fit([trainDict[key] for key in trainDict.keys()], y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

#test model 4 still overfitting a lot.
y_test = testDict['y_train']
del testDict['y_train']
y_pred = model_4.predict([testDict[key] for key in testDict.keys()], batch_size=32, verbose=1)

print(classification_report(y_test, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 4 different output weights and reducing the num of hidden layers
lstm_1 = create_lstm(shape=(500,300), hidden_unit=16, output=4)
lstm_2 = create_lstm(shape=(500,300), hidden_unit=32, output=8)
lstm_3 = create_lstm(shape=(500,300), hidden_unit=64, output=16)

combinedInput = concatenate([lstm_1.output, lstm_2.output, lstm_3.output])

x = Dense(16, activation='relu')(combinedInput)
x = Dense(1, activation='sigmoid')(x)

model_4 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input], outputs=x)

model_4.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_4.fit([trainDict[key] for key in trainDict.keys()], y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_4.predict([testDict[key] for key in testDict.keys()], batch_size=32, verbose=1)
print(classification_report(y_test, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 4 different output weights and further reducing the num of hidden layers
lstm_1 = create_lstm(shape=(500,300), hidden_unit=8, output=4)
lstm_2 = create_lstm(shape=(500,300), hidden_unit=16, output=8)
lstm_3 = create_lstm(shape=(500,300), hidden_unit=32, output=16)

combinedInput = concatenate([lstm_1.output, lstm_2.output, lstm_3.output])

x = Dense(16, activation='relu')(combinedInput)
x = Dense(1, activation='sigmoid')(x)

model_4 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input], outputs=x)

model_4.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_4.fit([trainDict[key] for key in trainDict.keys()], y_train, batch_size=32, epochs=5, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_4.predict([testDict[key] for key in testDict.keys()], batch_size=32, verbose=1)
print(classification_report(y_test, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 4 different output weights and further reducing the num of hidden layers
lstm_1 = create_lstm(shape=(500,300), hidden_unit=8, output=4)
lstm_2 = create_lstm(shape=(500,300), hidden_unit=16, output=8)
lstm_3 = create_lstm(shape=(500,300), hidden_unit=32, output=16)

combinedInput = concatenate([lstm_1.output, lstm_2.output, lstm_3.output])

x = Dense(8, activation='relu')(combinedInput)
x = Dense(1, activation='sigmoid')(x)

model_4 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input], outputs=x)

model_4.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_4.fit([trainDict[key] for key in trainDict.keys()], y_train, batch_size=16, epochs=5, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_4.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(y_test, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 5 data preprocessing
X_train = np.load(DATA_PATH + "/SIF_formatted_for_sequence_model_all_time.npy", allow_pickle=True)
y_train = np.load(DATA_PATH + "/sequence_model_y_formatted_all_time.npy", allow_pickle=True)

#get 5 days of data
data = create_multiday(X_train, y_train, num_days=5)
trainDict, testDict = train_test_split_sqs(data, ratio=0.05)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

#model 5: bi-LSTM feed into LSTM with reduced parameters this looks pretty good
lstm_1 = create_lstm(shape=(500,300), hidden_unit=8, output=4)
lstm_2 = create_lstm(shape=(500,300), hidden_unit=8, output=4)
lstm_3 = create_lstm(shape=(500,300), hidden_unit=12, output=4)
lstm_4 = create_lstm(shape=(500,300), hidden_unit=16, output=4)
lstm_5 = create_lstm(shape=(500,300), hidden_unit=16, output=4)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(3, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_5 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input], outputs=x)

model_5.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_5.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=5, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_5.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#try model 5 on BERT embedding
#get 5 days of data
data = create_multiday(X_train, y_train, num_days=5)
trainDict, testDict = train_test_split_sqs(data, ratio=0.05)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_lstm(hidden_unit=16, output=16)
lstm_2 = create_lstm(hidden_unit=32, output=16)
lstm_3 = create_lstm(hidden_unit=32, output=16)
lstm_4 = create_lstm(hidden_unit=64, output=16)
lstm_5 = create_lstm(hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(5, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_5 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input], outputs=x)

model_5.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_5.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=30, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_5.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#try model 5 on USE embedding
#get 5 days of data
data = create_multiday(X_train, y_train, num_days=5)
trainDict, testDict = train_test_split_sqs(data, ratio=0.05)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_3 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_4 = create_lstm(shape=(500,512), hidden_unit=64, output=16)
lstm_5 = create_lstm(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(5, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_5 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input], outputs=x)

model_5.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_5.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=30, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_5.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#try model 5 on USE embedding good enough results!
#get 5 days of data
data = create_multiday(X_train, y_train, num_days=5)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_3 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_4 = create_lstm(shape=(500,512), hidden_unit=64, output=16)
lstm_5 = create_lstm(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(5, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_5 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input], outputs=x)

model_5.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_5.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=30, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_5.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 6 on USE embedding good enough results!
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_3 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_4 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_5 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_6 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_7 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_8 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_9 = create_lstm(shape=(500,512), hidden_unit=64, output=16)
lstm_10 = create_lstm(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=30, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 6 on USE embedding good enough results!
#get 10 days of data

lstm_1 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_3 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_4 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_5 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_6 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_7 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_8 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_9 = create_lstm(shape=(500,300), hidden_unit=64, output=16)
lstm_10 = create_lstm(shape=(500,300), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=6, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 6 on USE embedding good enough results!
#get 10 days of data
#data = create_multiday(X_train, y_train, num_days=10)
#trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

#yTrain = trainDict['y_train']
#del trainDict['y_train']

#yTest = testDict['y_train']
#del testDict['y_train']

lstm_1 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_3 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_4 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_5 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_6 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_7 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_8 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_9 = create_lstm(shape=(500,512), hidden_unit=64, output=16)
lstm_10 = create_lstm(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=8, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 6 on SIF embedding good enough results!
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_3 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_4 = create_lstm(shape=(500,300), hidden_unit=16, output=16)
lstm_5 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_6 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_7 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_8 = create_lstm(shape=(500,300), hidden_unit=32, output=16)
lstm_9 = create_lstm(shape=(500,300), hidden_unit=64, output=16)
lstm_10 = create_lstm(shape=(500,300), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=8, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 6 on USE embedding good enough results! Use early stopping
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_3 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_4 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_5 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_6 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_7 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_8 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_9 = create_lstm(shape=(500,512), hidden_unit=64, output=16)
lstm_10 = create_lstm(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#add regularization
lstm_1 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_3 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_4 = create_lstm(shape=(500,512), hidden_unit=16, output=16)
lstm_5 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_6 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_7 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_8 = create_lstm(shape=(500,512), hidden_unit=32, output=16)
lstm_9 = create_lstm(shape=(500,512), hidden_unit=64, output=16)
lstm_10 = create_lstm(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 7 on USE embedding : test GRU models
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_gru(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_gru(shape=(500,512), hidden_unit=16, output=16)
lstm_3 = create_gru(shape=(500,512), hidden_unit=16, output=16)
lstm_4 = create_gru(shape=(500,512), hidden_unit=16, output=16)
lstm_5 = create_gru(shape=(500,512), hidden_unit=32, output=16)
lstm_6 = create_gru(shape=(500,512), hidden_unit=32, output=16)
lstm_7 = create_gru(shape=(500,512), hidden_unit=32, output=16)
lstm_8 = create_gru(shape=(500,512), hidden_unit=32, output=16)
lstm_9 = create_gru(shape=(500,512), hidden_unit=64, output=16)
lstm_10 = create_gru(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = GRU(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_6 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_6.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model_6.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_6.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 8 on USE embedding : test cnnLSTM models
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_cnnLSTM(shape=(500,512), hidden_unit=16, output=16)
lstm_2 = create_cnnLSTM(shape=(500,512), hidden_unit=16, output=16)
lstm_3 = create_cnnLSTM(shape=(500,512), hidden_unit=16, output=16)
lstm_4 = create_cnnLSTM(shape=(500,512), hidden_unit=16, output=16)
lstm_5 = create_cnnLSTM(shape=(500,512), hidden_unit=32, output=16)
lstm_6 = create_cnnLSTM(shape=(500,512), hidden_unit=32, output=16)
lstm_7 = create_cnnLSTM(shape=(500,512), hidden_unit=32, output=16)
lstm_8 = create_cnnLSTM(shape=(500,512), hidden_unit=32, output=16)
lstm_9 = create_cnnLSTM(shape=(500,512), hidden_unit=64, output=16)
lstm_10 = create_cnnLSTM(shape=(500,512), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_8 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_8.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model_8.summary()
history = model_8.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_8.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 8 on BERT embedding : test cnnLSTM models
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_cnnLSTM(hidden_unit=16, output=16)
lstm_2 = create_cnnLSTM(hidden_unit=16, output=16)
lstm_3 = create_cnnLSTM(hidden_unit=16, output=16)
lstm_4 = create_cnnLSTM(hidden_unit=16, output=16)
lstm_5 = create_cnnLSTM(hidden_unit=32, output=16)
lstm_6 = create_cnnLSTM(hidden_unit=32, output=16)
lstm_7 = create_cnnLSTM(hidden_unit=32, output=16)
lstm_8 = create_cnnLSTM(hidden_unit=32, output=16)
lstm_9 = create_cnnLSTM(hidden_unit=64, output=16)
lstm_10 = create_cnnLSTM(hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_8 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_8.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model_8.summary()
history = model_8.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1, shuffle=False)
plot_learning_curve(history)

y_pred = model_8.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#model 8 on BERT embedding : test cnnLSTM models
#get 10 days of data
data = create_multiday(X_train, y_train, num_days=10)
trainDict, testDict = train_test_split_sqs(data, ratio=0.2)

yTrain = trainDict['y_train']
del trainDict['y_train']

yTest = testDict['y_train']
del testDict['y_train']

lstm_1 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_2 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_3 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_4 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_5 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_6 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_7 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_8 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_9 = create_cnnLSTM(shape=(500,300), hidden_unit=64, output=16)
lstm_10 = create_cnnLSTM(shape=(500,300), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_8 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_8.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model_8.summary()
history = model_8.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1)
plot_learning_curve(history)

y_pred = model_8.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#enable shuffle
lstm_1 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_2 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_3 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_4 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16)
lstm_5 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_6 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_7 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_8 = create_cnnLSTM(shape=(500,300), hidden_unit=32, output=16)
lstm_9 = create_cnnLSTM(shape=(500,300), hidden_unit=64, output=16)
lstm_10 = create_cnnLSTM(shape=(500,300), hidden_unit=64, output=16)

output_list = [lstm_1.output, lstm_2.output, lstm_3.output, lstm_4.output, lstm_5.output, lstm_6.output, lstm_7.output, lstm_8.output, lstm_9.output, lstm_10.output]
sequenceInput = Lambda(lambda x: stack(x, axis=1))(output_list)

x = LSTM(10, dropout=0.2)(sequenceInput)
x = Dense(1, activation='sigmoid')(x)

model_8 = Model(inputs=[lstm_1.input, lstm_2.input, lstm_3.input, lstm_4.input, lstm_5.input, lstm_6.input, lstm_7.input, lstm_8.input, lstm_9.input, lstm_10.input], outputs=x)

model_8.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model_8.summary()
history = model_8.fit([trainDict[key] for key in trainDict.keys()], yTrain, batch_size=16, epochs=15, verbose=1, validation_split=0.1)
plot_learning_curve(history)

y_pred = model_8.predict([testDict[key] for key in testDict.keys()], batch_size=16, verbose=1)
print(classification_report(yTest, np.rint(y_pred.reshape(y_pred.shape[0]))))

#only using one layer of cnnLSTM model: SIF data
model_9 = create_cnnLSTM(shape=(500,300), hidden_unit=16, output=16, regress=True)
model_9.summary()
model_9.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
history = model_9.fit(X_train, y_train, batch_size=16, epochs=30, verbose=1, validation_split=0.1)
plot_learning_curve(history)

#only using one layer of cnnLSTM model: BERT data
model_9 = create_cnnLSTM(hidden_unit=16, output=16, regress=True)
model_9.summary()
model_9.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
history = model_9.fit(X_train, y_train, batch_size=16, epochs=30, verbose=1, validation_split=0.1)
plot_learning_curve(history)

#only using one layer of cnnLSTM model: USE data
model_9 = create_cnnLSTM(shape=(500,512), hidden_unit=16, output=16, regress=True)
model_9.summary()
model_9.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
history = model_9.fit(X_train, y_train, batch_size=16, epochs=30, verbose=1, validation_split=0.1)
plot_learning_curve(history)

